global:
  # Run Longhorn user-deployed components (manager/ui/driver) only on dedicated
  # storage nodes (Talos data-plane) that have the extra Longhorn disk
  nodeSelector:
    # This label is applied by Talos CCM (cluster-admin) to comply with
    # NodeRestriction (nodes cannot self-apply arbitrary labels)
    node.longhorn.io/create-default-disk: "true"

preUpgradeChecker:
  # GitOps/ArgoCD: disable pre-upgrade hooks and version checks (avoids hook Jobs)
  # Prevent manual upgrades via UI and sync conflicts; updates are managed via Git
  jobEnabled: false
  upgradeVersionCheck: false

defaultSettings:
  # Constrain Longhorn to storage nodes via `global.nodeSelector`
  # Gate default-disk creation on the CCM-applied label
  createDefaultDiskLabeledNodes: true

  # Store Longhorn data on the dedicated mount path on the nodes
  defaultDataPath: /var/mnt/longhorn

  # Prefer local replicas when possible
  defaultDataLocality: best-effort

  # Rebalance replicas in best-effort mode
  replicaAutoBalance: best-effort

  # Keep at least 15% free disk space on each Longhorn disk
  storageMinimalAvailablePercentage: 15

  # Reserve disk space on the default disk (headroom for CoW/fragmentation/ops)
  storageReservedPercentageForDefaultDisk: 25

  # Default number of replicas for volumes created via the UI
  defaultReplicaCount: 2

  # Reserve CPU for each instance manager pod (percent of allocatable CPU)
  guaranteedInstanceManagerCPU: 4

# Prometheus Metrics (Opt-In)
metrics:
  serviceMonitor:
    enabled: true
    labels:
      release: prometheus

# Reduce Longhorn UI replicas
longhornUI:
  replicas: 1
